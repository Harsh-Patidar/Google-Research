A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding.
There, unobserved variables may influence both the actions taken by the agent and the outcomes observed in the data.
Hidden confounding can compromise the validity of any causal conclusion drawn from the data and presents a major obstacle to effective offline RL.
In this paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due 
to confounding bias, termed delphic uncertainty, which uses variation over compatible world models, and differentiate it from the well
known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a 
pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts
to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis
management benchmark, as well as real electronic health records. 
Our results suggest that nonidentifiable confounding bias can be addressed in practice to improve offline RL solutions.
